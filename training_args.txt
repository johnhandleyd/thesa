Batch size: 8
Gradient accumulation steps: 1
Optimizer: paged_adamW_32bit
Learning rate: 2e-4
LR Scheduler Type: Cosine
Save: Epoch
Logging steps: 50
Num. epochs: 5
Max. steps: 250
FP16: True
Packing: False
Max. sequence length: 1024